#+title: Machine Learning
#+subtitle: Data first
#+author: Alberto Valdez
#+SETUPFILE: ../config/org-theme-alt.config
#+SETUPFILE: ../config/org-header.config

* Machine Learning
:PROPERTIES:
:CUSTOM_ID: machine-learning
:END:

ML is the use of statistical algorithm to perform tasks such as learning from data patterns and making predictions. There are different models (mathematical representations of real-world phenomena). It can be divided into: supervised, unsupervised and deep learning.

** Supervised Learning
:PROPERTIES:
:CUSTOM_ID: supervised-learning
:END:

Deals with labeled data, for example, determining wether a patient has diabetes or not.

| Age | BMI | Has Diabetes |
|-----+-----+--------------|
|  31 |  24 | No           |
|  68 |  39 | Yes          |
|  57 |  35 | ?            |

Based on data across different categories and measurements, we try to get an answer on the next patient. In this case, based on age and BMI.

** Unsupervised Learning
:PROPERTIES:
:CUSTOM_ID: unsupervised-learning
:END:

Work with datasets without labeled outcomes, so answers or labels as correct answers are not provided. One example can be grouping objects by shape in different clusters.

* Supervised Learning
:PROPERTIES:
:CUSTOM_ID: supervised-learning
:END:

It can be broadly divided into regression and classification.

** Regression
:PROPERTIES:
:CUSTOM_ID: regression
:END:

Is used to predict continuous variables. For example, predict a person weight based on other factors like height, diet, exercise.

** Classification
:PROPERTIES:
:CUSTOM_ID: classification
:END:

Is used to predict discrete outcomes. For example, predict if a person will vote on a particular issue depending on their age, sex, income, geographical location, etc. The outcome is either "Yes" or "No", the classification model would attempt to learn patterns from the data and gain accurate prediction from it.

** Regression vs. Classification
:PROPERTIES:
:CUSTOM_ID: regression-vs-classification
:END:

The difference between them is that regression tries to predict a continuous variable while classification tries to predict ordinal data.

In both, a dataset is divided into features and target. *Features* are variables used to make a prediction while *Target* is the predicted outcome.


* Linear Regression in Python
:PROPERTIES:
:CUSTOM_ID: linear-regression-in-python
:END:

** Load the Data
:PROPERTIES:
:CUSTOM_ID: load-the-data
:END:

#+begin_src python
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

resources = Path("../resources")
df = pd.read_csv(resources / "Salary_Data.csv")
print(df.head())
#+end_src

#+RESULTS[2e06f02725ec799a1e4d64b6c7f57a5bde228250]:
#+begin_example
   YearsExperience   Salary
0              1.1  39343.0
1              1.3  46205.0
2              1.5  37731.0
3              2.0  43525.0
4              2.2  39891.0
#+end_example

** Plot the Data
:PROPERTIES:
:CUSTOM_ID: plot-the-data
:END:

Now we can plot the data to get an idea of the distribution.

#+begin_src python :wrap org
output = resources / "salary1.png"
fig, ax = plt.subplots(figsize=(8, 5))
ax.scatter(df.YearsExperience, df.Salary)
ax.set_xlabel('Years of Experience')
ax.set_ylabel('Salary in USD')
fig.savefig(output)
print("#+attr_html: :width 500px")
print(f"[[{output}]]")
#+end_src

#+RESULTS[cd31bd744e2a98fb7c3df756de83dc745f5ecd4d]:
#+begin_org
#+attr_html: :width 500px
[[../resources/salary1.png]]
#+end_org

** Find a Relationship
:PROPERTIES:
:CUSTOM_ID: find-a-relationship
:END:

Normally, the independent variable is placed on the x-axis, and the dependent variable on the y-axis. The years of experience is independent because we assume that the salary depends on experience.

#+begin_src python
X = df.YearsExperience.values.reshape(-1, 1)
print("data:", X[:5])
print("shape:", X.shape)
#+end_src

#+RESULTS[1cc12775e9849a5d281d267b443825fa70c61ecd]:
#+begin_example
data: [[1.1]
 [1.3]
 [1.5]
 [2. ]
 [2.2]]
shape: (30, 1)
#+end_example

We have to reshape the array in order to fit with *Scikit-learn* standards. Our X variable remains the Years of Experience and the y variable will be the Salary.

#+begin_src python
y = df.Salary
print(y[:5])
#+end_src

#+RESULTS[b64f43c25d892d077b2119f41020e6d23206e4b2]:
#+begin_example
0    39343.0
1    46205.0
2    37731.0
3    43525.0
4    39891.0
Name: Salary, dtype: float64
#+end_example

** Create a model instance
:PROPERTIES:
:CUSTOM_ID: create-a-model-instance
:END:

Once the data is ready, we will fit/train the model. Then we can use the predict method to generate predictions. The model has created a line with 30 datapoints in X which we can use for getting a value in y.

#+begin_src python
model = LinearRegression()
model.fit(X, y)
# predict
y_pred = model.predict(X)
print(y_pred.shape)
#+end_src

#+RESULTS[37e9603a943155b354d07cda732c214e4edba412]:
#+begin_example
(30,)
#+end_example

** Plotting the predictions
:PROPERTIES:
:CUSTOM_ID: plotting-the-predictions
:END:

We are going to plot both the available data and the predictions from the model.

#+begin_src python :wrap org
output = resources / "salary2.png"
fig, ax = plt.subplots(figsize=(8, 5))
# using the model data
ax.scatter(X, y)
ax.plot(X, y_pred, color='red')
fig.savefig(output)
print("#+attr_html: :width 500px")
print(f"[[{output}]]")
#+end_src

#+RESULTS[dfa8a0489726acb1a017231c3838fcdf16bedac6]:
#+begin_org
#+attr_html: :width 500px
[[../resources/salary2.png]]
#+end_org

The linear prediction can extrapolate beyond the current data, however we are currently showing the linear regression against the current data.

If we wanted to inspect the model, we can always take a look at its properties.

#+begin_src python
print(model.coef_)
print(model.intercept_)
#+end_src

#+RESULTS[404bf390330b305df023bbe0a03ccdb62a228b11]:
#+begin_example
[9449.96232146]
25792.200198668696
#+end_example


* Logistic Regression
:PROPERTIES:
:CUSTOM_ID: logistic-regression
:END:

Logistic regression predicts binary outcomes. An example of logistic regression might be to decide wether to approve a credit card application or not. Multiple variables are assessed to arrive at one of two answers: to prove or to deny the application.

If a given probability is above a certain cutoff point, the sample data is assigned to that class. If the probability is less, it is assigned to another class.

** Logistic Regression in Python
:PROPERTIES:
:CUSTOM_ID: logistic-regression-in-python
:END:

#+begin_src python
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.datasets import make_blobs

X, y = make_blobs(centers=2, random_state=42)
print(f"Labels: {y[:10]}")
print(f"Data: {X[:10]}")
#+end_src

#+RESULTS[9aa7c79f7004556568afac7325a7c7e77093a6c4]:
#+begin_example
Labels: [0 1 0 1 1 0 1 1 0 0]
Data: [[-2.98837186  8.82862715]
 [ 5.72293008  3.02697174]
 [-3.05358035  9.12520872]
 [ 5.461939    3.86996267]
 [ 4.86733877  3.28031244]
 [-2.14780202 10.55232269]
 [ 4.91656964  2.80035293]
 [ 3.08921541  2.04173266]
 [-2.90130578  7.55077118]
 [-3.34841515  8.70507375]]
#+end_example

The centers argument specifies the number of clusters in the dataset, in this case we made two clusters. The random_state ensures reproducibility of the dataset, so it's pseudo-random.

The X variable contains the coordinates of each data point and the y contains information on the class of each data point. So each datapoint belongs to either 0 or 1.

#+begin_src python :wrap org
output = resources/"logistic1.png"
fig, ax = plt.subplots(figsize=(8, 5))

ax.scatter(X[:, 0], X[:, 1], c=y)

fig.savefig(output)
print("#+attr_html: :width 500px")
print(f"[[{output}]]")
#+end_src

#+RESULTS[0ddf98cf0859cb1348726f7502c03473e9215e91]:
#+begin_org
#+attr_html: :width 500px
[[../resources/logistic1.png]]
#+end_org

The clusters are very noticeable as they will be either purple or yellow and their positions in the plane are very distinct.

** Train and Test Sets
:PROPERTIES:
:CUSTOM_ID: train-and-test-sets
:END:

The model uses the training dataset to learn from it. The uses the testing dataset to assess its performance. If you use the entire dataset to train a model, you won't know how well the model will perform with unseen data.

#+begin_src python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    random_state=1,
    stratify=y
)
#+end_src

#+RESULTS[db615a1c2e37e8ffda7d551c482f036bddbf8ce8]:
#+begin_example
#+end_example

The =train_test_split=[fn:1] module takes X and y arguments and splits each into training and tests sets. A different random state number would distribute the rows of data differently. The =random_state= argument should not be used for training real models.

The =stratify= argument divides a dataset proportionally, for example if 60% of the dataset belongs to class A and the rest to class B, stratify ensures that the entire dataset is split into training tests with the same distribution. We should always consider against stratify if the classes are severely unbalanced or when the dataset is small.

** Logistic Regression Model
:PROPERTIES:
:CUSTOM_ID: logistic-regression-model
:END:

#+begin_src python
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(
    solver='lbfgs',
    random_state=1
)
print(classifier)
#+end_src

#+RESULTS[73090a30898ed883392438818ce8305daf8ea2a3]:
#+begin_example
LogisticRegression(random_state=1)
#+end_example

Training the model.

#+begin_src python :exports code
classifier.fit(X_train, y_train)
#+end_src

#+RESULTS[f7c82a8f226f4fecafb7ec46fb9cdfd358790435]:
#+begin_example
#+end_example

Validate the model. We will create predictions based on the X_test, then we create a dataframe of the predicted values and actual values.

Then we can validate the model, we evaluate its performance.

#+begin_src python
from sklearn.metrics import accuracy_score

predictions = classifier.predict(X_test)
pd.DataFrame({"Prediction": predictions, "Actual": y_test})

score = accuracy_score(y_test, predictions)
print(score)
#+end_src

#+RESULTS[b4786574daab1d9a2928646d4112acd85fc99a11]:
#+begin_example
1.0
#+end_example

If the score is 1.0, it means that every single observation in the testing set was predicted correctly by the model. All samples belonging to class 1 were correctly predicted and all samples belonging to class 0 were also correctly predicted.

However, an extremely high metric should raise suspicion of overfitting, which is when an instance in which the patterns picked up by a model are too specific to a specific dataset.

We are going to create a new data point and evaluate it.

#+begin_src python :wrap org
import numpy as np

new_data = np.array([[-2, 6]])

output = resources / "logistic2.png"

fig, ax = plt.subplots(figsize=(8, 5))
ax.scatter(X[:, 0], X[:, 1], c=y)
ax.scatter(new_data[0, 0], new_data[0, 1], c="r", marker="o", s=100)
fig.savefig(output)

print("#+attr_html: :width 500px")
print(f"[[{output}]]")
#+end_src

#+RESULTS[37880cf0711ed2a4f923242e78d5c51cfd281bda]:
#+begin_org
#+attr_html: :width 500px
[[../resources/logistic2.png]]
#+end_org

Here we make the prediction, our result will tell us to which class the new datapoint belongs to.

#+begin_src python
predictions = classifier.predict(new_data)
print(f"The new point was classified as: {predictions}")
#+end_src

#+RESULTS[ca132203b17feef72fe847a1beab6dd6bacc06a3]:
#+begin_example
The new point was classified as: [0]
#+end_example

** Predicting Diabetes
:PROPERTIES:
:CUSTOM_ID: predicting-diabetes
:END:

We can use logistic regression to predict diabetes.

#+begin_src python
from pathlib import Path
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression


data = Path('../resources/diabetes.csv')
df = pd.read_csv(data)
print(df.head())
#+end_src

#+RESULTS[fcb7fc87851dbbc3f8b154cfb51e1c62a5714c26]:
#+begin_example
   Pregnancies  Glucose  ...  Age  Outcome
0            6      148  ...   50        1
1            1       85  ...   31        0
2            8      183  ...   32        1
3            1       89  ...   21        0
4            0      137  ...   33        1

[5 rows x 9 columns]
#+end_example

Let's separate the outcome column from the rest. So the "Outcome" column is defined as the target (y) and the rest are X (features).

#+begin_src python
y = df["Outcome"]
X = df.drop(columns="Outcome")
#+end_src

#+RESULTS[547774516eed7d6b088ee89b0e8146686201fc99]:
#+begin_example
#+end_example

Now we can repeat the logistic regression steps with this dataset.

#+begin_src python :exports code
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    random_state=1,
    stratify=y
)
print(X_train.size, X_test.size)
#+end_src

#+RESULTS[6d271e4c57e4988a420d76f965da729875942780]:
#+begin_example
4608 1536
#+end_example

Then we initialize a model.

#+begin_src python
classifier = LogisticRegression(
    solver='lbfgs',
    max_iter=200, # upper limit of num of iter solver
    random_state=1
)
print(classifier)
#+end_src

#+RESULTS[fa0210be7503ce75d82074fd731b96f455192cf1]:
#+begin_example
LogisticRegression(max_iter=200, random_state=1)
#+end_example

Now we train the model.

#+begin_src python :exports code
classifier.fit(X_train, y_train)
print(vars(classifier))
#+end_src

#+RESULTS[64257cf5e290e603f4844bff4a15ac3199466830]:
#+begin_example
{'penalty': 'l2', 'dual': False, 'tol': 0.0001, 'C': 1.0, 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'random_state': 1, 'solver': 'lbfgs', 'max_iter': 200, 'multi_class': 'auto', 'verbose': 0, 'warm_start': False, 'n_jobs': None, 'l1_ratio': None, 'feature_names_in_': array(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
       'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'], dtype=object), 'n_features_in_': 8, 'classes_': array([0, 1]), 'n_iter_': array([188], dtype=int32), 'coef_': array([[ 1.29480661e-01,  3.38916660e-02, -1.58245030e-02,
        -6.55823074e-03,  1.79390934e-04,  1.00014902e-01,
         1.08868519e+00,  1.62358480e-02]]), 'intercept_': array([-8.55725072])}
#+end_example

Then we create predictions.

#+begin_src python
y_pred = classifier.predict(X_test)
results = pd.DataFrame({"Prediction": y_pred, "Actual": y_test}).reset_index(drop=True)
print(results.head())
#+end_src

#+RESULTS[8e88a27d819a8fc6d7d4f09059828f218982a322]:
#+begin_example
   Prediction  Actual
0           0       0
1           1       1
2           0       0
3           1       1
4           0       0
#+end_example

And finally we evaluate performance.

#+begin_src python
print(accuracy_score(y_test, y_pred))
#+end_src

#+RESULTS[da487235791d6351465b8a533237497128ca257f]:
#+begin_example
0.7760416666666666
#+end_example

** How does it work?
:PROPERTIES:
:CUSTOM_ID: how-does-it-work
:END:

Linear regression would not work for when we want a binary value, because the binary outcome in a graph would be either at 1.0 or 0.0, no in-between.

The probability outcome is not given by linear regression but rather by:

#+begin_quote
log(probability of admission/(1 - probability of admission))
#+end_quote

This results in an S-shaped curve that represents the probability of being admitted at a given test score.

This S-shaped curve, also called a sigmoid curve, can then be used to predict acceptance for new applicants.[fn:2]

*** The Logit Function
:PROPERTIES:
:CUSTOM_ID: the-logit-function
:END:

Logarithms are useful at stretching a distribution's ratio. In this case, as the value on the x-axis increases, its y-axis value increases rapidly. The illustration on the right shows that the curve is been straightened into a line after plotting the logarithms of the values, since logarithms undo exponents.

The logit function[fn:3] best describes this phenomena.


* Accuracy and Sensitivity
:PROPERTIES:
:CUSTOM_ID: accuracy-and-sensitivity
:END:

** Confusion Matrix
:PROPERTIES:
:CUSTOM_ID: confusion-matrix
:END:

Whenever a model fails to make a prediction, it will fall in one of the following categories.

|                | Predicted True | Predicted False |
|----------------+----------------+-----------------|
| Actually True  | TRUE POSITIVE  | FALSE NEGATIVE  |
| Actually False | FALSE POSITIVE | TRUE NEGATIVE   |

Depending on what the model is trying to precit, different outcomes may be damaging. When predicting a fraudulent transaction, a FALSE NEGATIVE won't be able to detect an actual fraud while a FALSE POSITIVE will consider a non-fraudulent transaction as fraudulent, which is not as severe as the former.

** Precision
:PROPERTIES:
:CUSTOM_ID: precision
:END:

A confusion matrix shows the probability of a prediction of falling into one of the possible bins.

Precision is the measure of how likely is that the prediction is actually true. It is measured by dividing the True Positives by All Positives.

|                | Predicted True | Predicted False |
|----------------+----------------+-----------------|
| Actually True  |             30 |              10 |
| Actually False |             20 |              40 |

Precision = TP/(TP + FP)

In this case = 30 / (30 + 20) = 0.6

** Sensitivity
:PROPERTIES:
:CUSTOM_ID: sensitivity
:END:

Sensitivity or recall is how likely is the test to create a correct prediction, being either True or False. It is measured by dividing True Positives by True Positives plus False Negatives.

Sensitivity = TP/(TP + FN)

In this case = 30 / (30 + 10) = 0.75

A test with high sensitivity means few false negatives even with a high number of false positives. We mostly care to not have false negatives.

** Tradeoff
:PROPERTIES:
:CUSTOM_ID: tradeoff
:END:

Depending on what the test is for, sensitivity may be more important that precision.


** F1 Score / Harmonic Mean
:PROPERTIES:
:CUSTOM_ID: f1-score-harmonic-mean
:END:

The harmonic mean / F1 Score is a single summary statistic of precision and sensitivity.

F1 Score = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)

A useful way to think about the F1 score is that a pronounced imbalance between sensitivity and precision will yield a low F1 score.

** Confusion Matrix in Practice
:PROPERTIES:
:CUSTOM_ID: confusion-matrix-in-practice
:END:

#+begin_src python
from sklearn.metrics import confusion_matrix, classification_report


matrix = confusion_matrix(y_test, y_pred)
print(matrix)
true_pos = matrix[0][0]
false_pos = matrix[0][1]
false_neg = matrix[1][0]
true_neg = matrix[1][1]

print("sensitivity:", true_pos / (true_pos + false_pos))
print("accuracy:", true_pos / (true_pos + false_neg))
#+end_src

#+RESULTS[8aefe46f096940eee7f1be43e2cd81c762b580e9]:
#+begin_example
[[113  12]
 [ 31  36]]
sensitivity: 0.904
accuracy: 0.7847222222222222
#+end_example

Print a report.

The precision for prediction of the nondiabetics and diabetics are in line with each other. However, the recall (sensitivity) for predicting diabetes is much lower than it is for predicting an absence of diabetes. The lower recall for diabetics is reflected in the dropped F1 score as well

#+begin_src python
report = classification_report(y_test, y_pred)
print(report)
#+end_src

#+RESULTS[f87f95e97f6efd6b6b6117b9545a6f84d8be9fad]:
#+begin_example
              precision    recall  f1-score   support

           0       0.78      0.90      0.84       125
           1       0.75      0.54      0.63        67

    accuracy                           0.78       192
   macro avg       0.77      0.72      0.73       192
weighted avg       0.77      0.78      0.77       192
#+end_example


* Support Vector Machine
:PROPERTIES:
:CUSTOM_ID: support-vector-machine
:END:

Support Vector Machines (SVMs) are binary classifiers. It is similar to logistic regression, however, the goal of SVM is to find a line that separates the data into two classes. SVM draws a line at the edge of each class, and attempts to maximize the distance between them. It does so by separating the data points with the largest possible margins.

The hyperplanes need the widest equidistant margins possible. This improves classification predictions. The width of the margin is considered the margin of separation.

Support vectors are the data points closest to the hyperplane. They serve as decision boundaries for classification.

However, when there is an outlier, we can use soft margins to accomodate them, as they allow SVMs to make allowances.

In a 3D plane, the hyperplane would need to consider the another dimension to separate the both classes.

** SVMs in practice
:PROPERTIES:
:CUSTOM_ID: svms-in-practice
:END:

We are going to start with a model that has already been scaled.

#+begin_src python
data = Path('../resources/loans.csv')
df = pd.read_csv(data)
print(df.head())
#+end_src

#+RESULTS[c76bdcf658559b373c4cd36190c299fbecb43ce7]:
#+begin_example
     assets  liabilities  ...  mortgage   status
0  0.210859     0.452865  ...  0.302682     deny
1  0.395018     0.661153  ...  0.502831  approve
2  0.291186     0.593432  ...  0.315574  approve
3  0.458640     0.576156  ...  0.394891  approve
4  0.463470     0.292414  ...  0.566605  approve

[5 rows x 6 columns]
#+end_example

Then we select the data and split it for training.

#+begin_src python
y = df["status"]
X = df.drop(columns="status")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)
print(X_test.shape)
print(X_train.shape)
#+end_src

#+RESULTS[f9ba2315f2143fde8b7de31c8a8a849da4a288a7]:
#+begin_example
(25, 5)
(75, 5)
#+end_example

Then we import the model from sklearn and we train it with fit.

#+begin_src python
from sklearn.svm import SVC
model = SVC(kernel='linear')

model.fit(X_train, y_train)
#+end_src

#+RESULTS[87071ca7243582b35059baece4a634023b84e4de]:
#+begin_example
#+end_example

Finally we create the predictions.

#+begin_src python
y_pred = model.predict(X_test)
results = pd.DataFrame({
    "Prediction": y_pred,
    "Actual": y_test
}).reset_index(drop=True)
print(results.head())
#+end_src

#+RESULTS[88aca5838d60470a419c7543a6d7e4aa7e46f7fe]:
#+begin_example
  Prediction   Actual
0    approve     deny
1       deny  approve
2       deny     deny
3    approve     deny
4       deny     deny
#+end_example

Then we get the accuracy score and generate a prediction matrix.

#+begin_src python
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_pred)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
#+end_src

#+RESULTS[518a340b268065aaaf4b069ca25cbcd8e49a0a1a]:
#+begin_example
              precision    recall  f1-score   support

     approve       0.58      0.58      0.58        12
        deny       0.62      0.62      0.62        13

    accuracy                           0.60        25
   macro avg       0.60      0.60      0.60        25
weighted avg       0.60      0.60      0.60        25
#+end_example

The workflow of a SVM is very similar to a logistic regression:

1. Select the data (independent and dependent).
2. Split the data for training.
3. Create and train the model.
4. Create predictions.
5. Validate the model.

** Comparing SVM to Logistic Regression
:PROPERTIES:
:CUSTOM_ID: comparing-svm-to-logistic-regression
:END:

#+begin_src python
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(
    solver='lbfgs',
    random_state=1
)
classifier.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred = classifier.predict(X_test)
report = classification_report(y_test, y_pred)
print(report)
#+end_src

#+RESULTS[234e4d0a61f3d9ba8e0afbe1ceecfa335020e923]:
#+begin_example
              precision    recall  f1-score   support

     approve       0.50      0.25      0.33        12
        deny       0.53      0.77      0.62        13

    accuracy                           0.52        25
   macro avg       0.51      0.51      0.48        25
weighted avg       0.51      0.52      0.48        25
#+end_example

SVM wins.



* Data Pre-processing
:PROPERTIES:
:CUSTOM_ID: data-pre-processing
:END:

** Encoding Labels
:PROPERTIES:
:CUSTOM_ID: encoding-labels
:END:

Categorical and text data must be converted to numerical data for use in machine learning. From the following dataset we want to use the "Bad" column.

#+begin_src python
import pandas as pd
from pathlib import Path

filepath = Path("../resources/loans_data.csv")
loans_df = pd.read_csv(filepath)
print(loans_df.head())
#+end_src

#+RESULTS[7e2a9a2c0c616775bed652aa35758dcacf42b670]:
#+begin_example
   amount  term  ...  gender  bad
0    1000    30  ...    male    0
1    1000    30  ...  female    0
2    1000    30  ...  female    0
3    1000    15  ...    male    0
4    1000    30  ...  female    0

[5 rows x 7 columns]
#+end_example

We will encode the data from text to numbers using panda's =get_dummies= function.[fn:4]

#+begin_src python
loans_binary_encoded = pd.get_dummies(loans_df, columns=["gender"])
loans_binary_encoded = pd.get_dummies(loans_df, columns=["education", "gender"])
print(loans_binary_encoded.head())
#+end_src

#+RESULTS[9bd06e1881444e6f3580713084e2f9a3b86155ae]:
#+begin_example
   amount  term  ... gender_female  gender_male
0    1000    30  ...             0            1
1    1000    30  ...             1            0
2    1000    30  ...             1            0
3    1000    15  ...             0            1
4    1000    30  ...             1            0

[5 rows x 11 columns]
#+end_example

We can also use scikit-learn for encoding.

#+begin_src python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df2 = loans_df.copy()
df2['education'] = le.fit_transform(df2['education'])
df2['gender'] = le.fit_transform(df2['gender'])
print(df2.head())
#+end_src

#+RESULTS[324fe8625d87dcbb14cc05f492b9b4f787fb9edc]:
#+begin_example
   amount  term      month  age  education  gender  bad
0    1000    30       June   45          1       1    0
1    1000    30       July   50          0       0    0
2    1000    30     August   33          0       0    0
3    1000    15  September   27          3       1    0
4    1000    30    October   28          3       0    0
#+end_example

We can also create a custom encoding. In this case we use a dictionary as a lookup table and apply it to the entire month column via lambda.

#+begin_src python
months_num = {
   "January": 1,
   "February": 2,
   "March": 3,
   "April": 4,
   "May": 5,
   "June": 6,
   "July": 7,
   "August": 8,
   "September": 9,
   "October": 10,
   "November": 11,
   "December": 12,
}
loans_df['month'] = loans_df['month'].apply(lambda x: months_num[x])
print(loans_df.head())
#+end_src

#+RESULTS[93e0242e089c512ef83e1da98d0d94b60abf6bf8]:
#+begin_example
   amount  term  month  ...             education  gender bad
0    1000    30      6  ...  High School or Below    male   0
1    1000    30      7  ...              Bachelor  female   0
2    1000    30      8  ...              Bachelor  female   0
3    1000    15      9  ...               college    male   0
4    1000    30     10  ...               college  female   0

[5 rows x 7 columns]
#+end_example

** Scale and Normalize Data
:PROPERTIES:
:CUSTOM_ID: scale-and-normalize-data
:END:

Finally, we are going to make sure all the numeric values are in the same scale.

#+begin_src python
import pandas as pd
from pathlib import Path

file_path = Path("../resources/loans_data_encoded.csv")
encoded_df = pd.read_csv(file_path)
print(encoded_df.head())
#+end_src

#+RESULTS[36a3889cfb72cf7f0c8ac07d544e52290f0af8b7]:
#+begin_example
   amount  term  ...  gender_female  gender_male
0    1000    30  ...              0            1
1    1000    30  ...              1            0
2    1000    30  ...              1            0
3    1000    15  ...              0            1
4    1000    30  ...              1            0

[5 rows x 11 columns]
#+end_example

#+begin_src python
from sklearn.preprocessing import StandardScaler

data_scaler = StandardScaler()
loans_data_scaled = data_scaler.fit_transform(encoded_df)
print(loans_data_scaled[:5])
#+end_src

#+RESULTS[87b017bf0ea9054f1fade5c1a1c069a36671c9c1]:
#+begin_example
[[ 0.49337687  0.89789115  2.28404253 -0.81649658 -0.16890147 -0.39336295
   1.17997648 -0.08980265 -0.88640526 -0.42665337  0.42665337]
 [ 0.49337687  0.89789115  3.10658738 -0.81649658  0.12951102  2.54218146
  -0.84747452 -0.08980265 -0.88640526  2.34382305 -2.34382305]
 [ 0.49337687  0.89789115  0.3099349  -0.81649658  0.42792352  2.54218146
  -0.84747452 -0.08980265 -0.88640526  2.34382305 -2.34382305]
 [ 0.49337687 -0.97897162 -0.67711892 -0.81649658  0.72633602 -0.39336295
  -0.84747452 -0.08980265  1.12815215 -0.42665337  0.42665337]
 [ 0.49337687  0.89789115 -0.51260995 -0.81649658  1.02474851 -0.39336295
  -0.84747452 -0.08980265  1.12815215  2.34382305 -2.34382305]]
#+end_example

#+begin_src python
dft = pd.DataFrame(loans_data_scaled, columns=encoded_df.columns)
print(dft.describe())
#+end_src

#+RESULTS[9f7ca6185fa79f0733e8513c45d7fa8e860ccaba]:
#+begin_example
             amount  ...   gender_male
count  5.000000e+02  ...  5.000000e+02
mean  -3.552714e-16  ...  3.907985e-17
std    1.001002e+00  ...  1.001002e+00
min   -5.586972e+00  ... -2.343823e+00
25%    4.933769e-01  ...  4.266534e-01
50%    4.933769e-01  ...  4.266534e-01
75%    4.933769e-01  ...  4.266534e-01
max    4.933769e-01  ...  4.266534e-01

[8 rows x 11 columns]
#+end_example

* Decision Trees
:PROPERTIES:
:CUSTOM_ID: decision-trees
:END:

Decision trees describe a sequence of optional values and are formed by root, nodes, splitting and leafs. The root is the first node in the tree, then we split it into different nodes, one for each decision, and so on, whenever we reach a node with no children, we call that a leaf.

** Scaling the Data
:PROPERTIES:
:CUSTOM_ID: scaling-the-data
:END:

#+begin_src python
import pandas as pd
from pathlib import Path
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

filepath = Path("../resources/loans_data_encoded.csv")
df_loans = pd.read_csv(filepath)
print(df_loans.head())
#+end_src

#+RESULTS[10a05f2cf415b507fb15c9f728a5f20e12d0f8af]:
#+begin_example
   amount  term  ...  gender_female  gender_male
0    1000    30  ...              0            1
1    1000    30  ...              1            0
2    1000    30  ...              1            0
3    1000    15  ...              0            1
4    1000    30  ...              1            0

[5 rows x 11 columns]
#+end_example

Our goal is to predict if a loan application is worthy of approval based on information we have in our =df_loans= df. So we will split the dataset into features (or inputs) and target (or outputs).

#+begin_src python
X = df_loans.copy()
X = X.drop("bad", axis=1)
y = df_loans["bad"].values
print(y[:10])
#+end_src

#+RESULTS[506b0fc766f214ed7593e6a37895dbf1a65ba154]:
#+begin_example
[0 0 0 0 0 0 0 0 0 0]
#+end_example

Then we split the data for training.

#+begin_src python
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
#+end_src

#+RESULTS[8dd81c0cd4bf906ef3092dac8053fdb9ee6d6bc1]:
#+begin_example
(375, 10)
(125, 10)
(375,)
(125,)
#+end_example

Alternatively, we can give a different train to test ratio[fn:5].

#+begin_src python
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=78, train_size=0.80)
print(X_train2.shape)
print(X_test2.shape)
print(y_train2.shape)
print(y_test2.shape)
#+end_src

#+RESULTS[e31aa7b812207c95a14bea90eafa9cc7321c707a]:
#+begin_example
(400, 10)
(100, 10)
(400,)
(100,)
#+end_example

We will use the standard scaler to normalize the data.

#+begin_src python
scaler = StandardScaler()
X_scaler = scaler.fit(X_train)

X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

import numpy as np
df_train = pd.DataFrame(X_train_scaled, columns = X.columns)
df_test = pd.DataFrame(X_test_scaled, columns = X.columns)
print(df_train.describe())
print(df_test.describe())
#+end_src

#+RESULTS[164363674dd69f53fbc5c11edd200fdb6a8d4217]:
#+begin_example
             amount  ...   gender_male
count  3.750000e+02  ...  3.750000e+02
mean   3.931670e-16  ... -9.000208e-17
std    1.001336e+00  ...  1.001336e+00
min   -5.367022e+00  ... -2.386719e+00
25%    7.705463e-02  ...  4.189852e-01
50%    4.958298e-01  ...  4.189852e-01
75%    4.958298e-01  ...  4.189852e-01
max    4.958298e-01  ...  4.189852e-01

[8 rows x 10 columns]
           amount        term  ...  gender_female  gender_male
count  125.000000  125.000000  ...     125.000000   125.000000
mean     0.080405    0.122970  ...       0.052373    -0.052373
std      0.848449    0.960249  ...       1.053179     1.053179
min     -5.367022   -1.930833  ...      -0.418985    -2.386719
25%      0.495830   -0.939137  ...      -0.418985     0.418985
50%      0.495830    0.920294  ...      -0.418985     0.418985
75%      0.495830    0.920294  ...      -0.418985     0.418985
max      0.495830    0.920294  ...       2.386719     0.418985

[8 rows x 10 columns]
#+end_example

** Use the Decision Tree Model
:PROPERTIES:
:CUSTOM_ID: use-the-decision-tree-model
:END:

#+begin_src python
model = tree.DecisionTreeClassifier()
model.fit(X_train_scaled, y_train)
predictions = model.predict(X_test_scaled)
print(predictions)
#+end_src

#+RESULTS[ebf6a2c98599a2ffc2f51a773c7db321d3e04634]:
#+begin_example
[1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0
 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0
 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0
 0 1 0 1 0 1 1 0 0 1 0 0 1 0]
#+end_example

Now we evaluate the model.

#+begin_src python
cm = confusion_matrix(y_test, predictions)
cm_df = pd.DataFrame(
    cm,
    index=["Actual 0", "Actual 1"],
    columns=["Predicted 0", "Predicted 1"],
)
print(cm_df)
#+end_src

#+RESULTS[f7243edc44dd7d568a43dad42a583ba38f6f54f9]:
#+begin_example
          Predicted 0  Predicted 1
Actual 0           50           34
Actual 1           21           20
41 20
sensitivity 0.4878048780487805
#+end_example

#+begin_src python
trues = cm[0][0] + cm[1][0]
positives = cm[0][0] + cm[0][1]
true_positive = cm[0][0]
false_positive = cm[0][1]

print("trues, positives:", trues, positives)
acc_score = accuracy_score(y_test, predictions)
print("accuracy:", acc_score)
print("precision:", true_positive / trues)
print("recall", true_positive / positives)
print("Classification Report\n" +  classification_report(y_test, predictions))
#+end_src

#+RESULTS[230367bf85d27e284ea54ba0b34d6f7d1e921a7d]:
#+begin_example
trues, positives: 71 84
accuracy: 0.56
precision: 0.704225352112676
recall 0.5952380952380952
Classification Report
              precision    recall  f1-score   support

           0       0.70      0.60      0.65        84
           1       0.37      0.49      0.42        41

    accuracy                           0.56       125
   macro avg       0.54      0.54      0.53       125
weighted avg       0.59      0.56      0.57       125
#+end_example

In resume, precision is the measure of how reliable a positive classification is. The precision of a good load application in this case is 0.56. The precision of a bad loan application is 0.358. A low precision is indicative of a large number of false positives[fn:6].

The model is not good enough because the precision is too low! (0.56).

* Ensemble Learning
:PROPERTIES:
:CUSTOM_ID: ensemble-learning
:END:

The concept of ensemble learning is the process of combining multiple models, like decision tree algorithms, to help improve the accuracy and robustness, and decrease variance of the model.

Weak learnes are algorithms that will make inaccurate and imprecise predictions because they are poor at learning adequately as result of limited data. However, weak learners should not be considered unworthy as there are models that can combine many weak learners to create a more accurate and robust prediction engine.

If we combine a decision tree that is a weak learner (low accuracy) with other trees we may get a more accurate prediction. The algorithms for combining them can be Random Forests, GradientBoostedTree and XGBoost.

** Random Forests
:PROPERTIES:
:CUSTOM_ID: random-forests
:END:

A random forest is an algorithm that samples the data and builds several smaller, simpler decision trees, each tree is simpler because is built from a random subset of features.

The trees are fed smaller samples of the data and because the sample is small, they are only slightly better than a random guess. However, many of them can combine for creating a strong learner.

Random forest are resilient to overfitting as the data is ditributed, they can also be used to rank the importance of input variables in a natural way. They can handle thousands of input variables and are robust to outliers and nonlinear data. They are also efficient on large dataset.

We will use the ensemble module and the RandomForestClassifier class.

#+begin_src python
import pandas as pd
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

file_path = Path("../resources/loans_data_encoded.csv")
df_loans = pd.read_csv(file_path)
print(df_loans.head())
#+end_src

#+RESULTS[b7a8344a661d7b4ea66e2c5d75db937fe2cb0664]:
#+begin_example
   amount  term  ...  gender_female  gender_male
0    1000    30  ...              0            1
1    1000    30  ...              1            0
2    1000    30  ...              1            0
3    1000    15  ...              0            1
4    1000    30  ...              1            0

[5 rows x 11 columns]
#+end_example

We will prepare the data.

#+begin_src python
X = df_loans.copy()
X = X.drop("bad", axis=1)
y = df_loans["bad"].ravel()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
#+end_src

#+RESULTS[993396d4d6e6e5bb8ac4cb24e9e65307182b0053]:
#+begin_example
(375, 10) (125, 10) (375,) (125,)
#+end_example

Then we scale it.

#+begin_src python
scaler = StandardScaler()
X_scaler = scaler.fit(X_train)
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
print(X_train_scaled[:5])
#+end_src

#+RESULTS[dce687f7b08e80acfca4e70806f659ffb6f36e99]:
#+begin_example
[[ 0.49582979 -0.93913666  0.62959256  1.26612714  2.64172735 -0.811968
  -0.10383483 -0.93541435  2.38671921 -2.38671921]
 [ 0.49582979  0.92029442 -0.69123099  1.56549516 -0.3785402  -0.811968
  -0.10383483  1.06904497 -0.41898519  0.41898519]
 [ 0.49582979  0.92029442 -1.18653982  0.96675912 -0.3785402   1.23157563
  -0.10383483 -0.93541435 -0.41898519  0.41898519]
 [ 0.49582979  0.92029442 -0.85633394  0.06865507 -0.3785402  -0.811968
  -0.10383483  1.06904497  2.38671921 -2.38671921]
 [-1.17927084 -0.93913666 -0.69123099  1.56549516 -0.3785402  -0.811968
  -0.10383483  1.06904497  2.38671921 -2.38671921]]
#+end_example

Now we create and fit the Random Forest Model[fn:7]. The best practice is to use between 64 and 128 random forest although they can be higher.

#+begin_src python
rf_model = RandomForestClassifier(n_estimators=128, random_state=78)
rf_model = rf_model.fit(X_train_scaled, y_train)
predictions = rf_model.predict(X_test_scaled)
print(predictions[:10])
#+end_src

#+RESULTS[e034bd0aa11933d676bcff8a6ac5027e590b3085]:
#+begin_example
[0 0 1 0 1 0 0 0 0 0]
#+end_example

Evaluate the model.

#+begin_src python
cm = confusion_matrix(y_test, predictions)
cm_df = pd.DataFrame(
    cm,
    index=["Actual 0", "Actual 1"],
    columns=["Predicted 0", "Predicted 1"],
)
print(cm_df)
print("accuracy", accuracy_score(y_test, predictions))
print(classification_report(y_test, predictions))
#+end_src

#+RESULTS[58f57c25e3830aa30448e4cefee381f3fbafa87d]:
#+begin_example
          Predicted 0  Predicted 1
Actual 0           51           33
Actual 1           23           18
accuracy 0.552
              precision    recall  f1-score   support

           0       0.69      0.61      0.65        84
           1       0.35      0.44      0.39        41

    accuracy                           0.55       125
   macro avg       0.52      0.52      0.52       125
weighted avg       0.58      0.55      0.56       125
#+end_example

We got lof precision and low F1 score so the model is not good.

However, we can rank the importance of the features. If we inspect the model, we can get an array of scores for the features in the =X_test= set, whose sum equals 1.0.

#+begin_src python
importances = rf_model.feature_importances_
print(importances)
#+end_src

#+RESULTS[e68990f7fa9c4442a857e51827e89b22c78d0d8d]:
#+begin_example
[0.05454782 0.07997292 0.43280448 0.32973986 0.01887172 0.02110219
 0.00271658 0.02151063 0.01887818 0.01985562]
#+end_example

Each number represents a column. So if we sort them together, we can get a better description of what features are more important.

#+begin_src python
importances_sorted = sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)
print(importances_sorted)
#+end_src

#+RESULTS[3d60903e49fc8c1629b9796f2df26884dac6eccf]:
#+begin_example
[(0.43280447750315343, 'age'), (0.32973986443922343, 'month_num'), (0.07997292251445517, 'term'), (0.05454782107242418, 'amount'), (0.021510631303272416, 'education_college'), (0.021102188881175144, 'education_High School or Below'), (0.01985561654170213, 'gender_male'), (0.018878176828577283, 'gender_female'), (0.018871722006693077, 'education_Bachelor'), (0.002716578909323729, 'education_Master or Above')]
#+end_example

However, because this is a random forest model, dropping low ranking features won't help us improve the model.

* Bootstrap aggregation
:PROPERTIES:
:CUSTOM_ID: bootstrap-aggregation
:END:

Bootstrapping is a sampling technique in which samples are randomly selected, then returned to the general pool and replaced, or put back into the general pool. Examples:

Sample 1: A, A, A, B, D
Sample 2: A, B, B, C, E
Sample 3: B, C, D, D, E

We can have the same datapoint appearing multiple times in the same sample. In summary is a technique with which a number of samples are made and in which an observation can occur multiple times.

** Aggregation
:PROPERTIES:
:CUSTOM_ID: aggregation
:END:

In each aggregation step, different classifiers are run, using the samples drawn in the bootstrapping stage. Each classifier will vote for a label (a prediction). The final prediction is the one with the most votes.

So the order is Dataset -> Sample N -> Prediction N -> Vote, where each Sample has a Prediction and the results of the predictions are counted and output as a Final Prediction.

** Boosting
:PROPERTIES:
:CUSTOM_ID: boosting
:END:

Like bagging, boosting is a technique to combine a set of weak learners into a strong learner. We saw in bagging that the different models work independently of one another. In contrast, boosting trains a sequence of weak models.

So instead of a parallel process we have a series of Sample -> Prediction in which the weakness of each model is passed to the next training set, then all models are combined for an ensemble prediction.

** Adaptive Boosting
:PROPERTIES:
:CUSTOM_ID: adaptive-boosting
:END:

In AdaBoost, a model is trained then evaluated. After evaluating the errors of the first model, another model is trained. This time, however, the model gives extra weight to the errors from the previous model, so the subsequent models minimize similar errors. This process is repeated until the error rate is minimized.

** Gradient Boosting
:PROPERTIES:
:CUSTOM_ID: gradient-boosting
:END:

It is an ensemble method that works sequentially. Gradient boosting does not seek to minimize errors by adjusting the weight of the errors, but it rather does the following:

1. A small tree (stump) is added to the model and the errors are evaluated.
2. A second stump is added to the first and attempts to minimize the errors from the first stump. These errors are called pseudo-residuals.
3. A third stump is added to the first two and attempts to minimize the pseudo-residuals from the previous two.
4. The process is repeated until the errors are minimized as much as possible or until a specified number of repetitions has been reached.

The learning rate refers to how aggressively pseudo-residuals are corrected during each iteration. In general, it is preferable to begin with a lower learning rate and, if necessary, adjust the rate updward.

** Boosting in Practice
:PROPERTIES:
:CUSTOM_ID: boosting-in-practice
:END:

#+begin_src python
import pandas as pd
from pathlib import Path

filepath = Path("../resources/loans_data_encoded.csv")
loans_df = pd.read_csv(filepath)
print(loans_df.head())
#+end_src

#+RESULTS[de0ca7cfa0abc61db03ac2875ecb205f6c04d4ba]:
#+begin_example
   amount  term  ...  gender_female  gender_male
0    1000    30  ...              0            1
1    1000    30  ...              1            0
2    1000    30  ...              1            0
3    1000    15  ...              0            1
4    1000    30  ...              1            0

[5 rows x 11 columns]
#+end_example

We prepare the data.

#+begin_src python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = loans_df.copy()
X = X.drop("bad", axis=1)
y = loans_df["bad"].values
# split
X_train, X_test, y_train, y_test = train_test_split(X,
   y, random_state=1)
# scale
scaler = StandardScaler()
X_scaler = scaler.fit(X_train)
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
#+end_src

We will identify the learning rate that yield the best performance.

#+begin_src python
from sklearn.ensemble import GradientBoostingClassifier

learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]
for rate in learning_rates:
    classifier = GradientBoostingClassifier(
        n_estimators=20,
        learning_rate=rate,
        max_features=5,
        max_depth=3,
        random_state=0,
    )
    classifier.fit(X_train_scaled, y_train.ravel())

    # results
    print("Learning rate: ", rate)
    print(
        "Accuracy score (training): {0:.3f}"
        .format(
            classifier.score(
                X_train_scaled,
                y_train)
        ))
    print(
       "Accuracy score (validation): {0:.3f}"
       .format(
           classifier.score(
               X_test_scaled,
               y_test)
       ))
#+end_src

#+RESULTS[2554664c6322e24d6ddbafbc88f5e40da2f114c1]:
#+begin_example
Learning rate:  0.05
Accuracy score (training): 0.611
Accuracy score (validation): 0.632
Learning rate:  0.1
Accuracy score (training): 0.653
Accuracy score (validation): 0.584
Learning rate:  0.25
Accuracy score (training): 0.720
Accuracy score (validation): 0.536
Learning rate:  0.5
Accuracy score (training): 0.773
Accuracy score (validation): 0.544
Learning rate:  0.75
Accuracy score (training): 0.784
Accuracy score (validation): 0.568
Learning rate:  1
Accuracy score (training): 0.835
Accuracy score (validation): 0.560
#+end_example

We get the training and testing results. A model that performs well on the training set but poorply on the testing set is said to be "overfit". In this case the case with a learning rate of 0.05 is best because of the high validation score.

Now we can use that model for our gradient boosting.

#+begin_src python
classifier = GradientBoostingClassifier(
    n_estimators=20,
    learning_rate=0.5,
    max_features=5,
    max_depth=3,
    random_state=0
)
classifier.fit(X_train_scaled, y_train)
predictions = classifier.predict(X_test_scaled)
print(predictions[:5])
#+end_src

#+RESULTS[73745cd620ae192207f3a4fc30a64f88d10d4ad3]:
#+begin_example
[1 0 1 0 1]
#+end_example

Then we assess the performance of the model.

#+begin_src python
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

acc_score = accuracy_score(y_test, predictions)
cm = confusion_matrix(y_test, predictions)
cm_df = pd.DataFrame(
   cm, index=["Actual 0", "Actual 1"],
   columns=["Predicted 0", "Predicted 1"]
)
print(f"Accuracy Score : {acc_score}")
print(cm_df)
print("Classification Report")
print(classification_report(y_test, predictions))
#+end_src

#+RESULTS[78041f88689e5afe5050076c3eb7a47136469ec1]:
#+begin_example
Accuracy Score : 0.544
          Predicted 0  Predicted 1
Actual 0           53           31
Actual 1           26           15
Classification Report
              precision    recall  f1-score   support

           0       0.67      0.63      0.65        84
           1       0.33      0.37      0.34        41

    accuracy                           0.54       125
   macro avg       0.50      0.50      0.50       125
weighted avg       0.56      0.54      0.55       125
#+end_example

The model has a slightly higher score but is still too low for using it.


* Oversampling
:PROPERTIES:
:CUSTOM_ID: oversampling
:END:

Class imbalance is a common problem in classification, it occurs when one class is much larger than the other class. In this case, the non-fraudulent class is much larger than the fraudulent class. The existing classes in the dataset aren't equally represented.

This can cause ML models to be biased toward the majority class, in this case, the model will be much better predicting non-fraudulent transactions than fraudulent ones.

We can use oversampling to increase the instances of one class in the dataset. We choose more instances from that class for training until it's larger.

** Random Oversampling
:PROPERTIES:
:CUSTOM_ID: random-oversampling
:END:

Instances of the minority class are randomly selected and added to the training set until the majority and minority classes are balanced.

#+begin_src python :results file :wrap org
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from collections import Counter

imgpath = "../resources/oversampling1.png"
X, y = make_blobs(
    n_samples=[600, 60],
    random_state=1,
    cluster_std=5
)
fig, ax = plt.subplots(figsize=(5, 4))
ax.scatter(X[:, 0], X[:, 1], c=y)
fig.savefig(imgpath)
print(imgpath)
#+end_src

#+RESULTS[42ae840fd800b32c3ba662c54d77f267ef9580cd]:
#+begin_org
[[file:../resources/oversampling1.png]]
#+end_org

We can see that the purple class visibly outnumbers the yellow class.

#+begin_src python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
print(Counter(y_train))
#+end_src

#+RESULTS[119cf62f5151227ccea1022b87fe9da7b8ed31f6]:
#+begin_example
Counter({0: 451, 1: 44})
#+end_example

We can use =imblearn= for oversampling. The data is resampled using the =fit_resample= method.

#+begin_src python
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=1)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
print(X_resampled.shape, y_resampled.shape)
#+end_src

#+RESULTS[e4ec1ed57ec61e6708ba885b9d5bb1976e8a02d2]:
#+begin_example
(902, 2) (902,)
#+end_example

Now that the data is ready, we can use a model for making predictions.

#+begin_src python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix


model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
#+end_src

#+RESULTS[25ce8d684b8db54ea3468a06f7654b073ad0aef3]:
#+begin_example
[[131  18]
 [  1  15]]
#+end_example

We are going to use a balanced accuracy score instead of a regular one, as well as a classification report imbalanced.

#+begin_src python
from sklearn.metrics import balanced_accuracy_score
from imblearn.metrics import classification_report_imbalanced

print("accuracy:", balanced_accuracy_score(y_test, y_pred))
print(classification_report_imbalanced(y_test, y_pred))
#+end_src

#+RESULTS[ba8d625c33aba7ac4b5fb20a743dbb9b496e2eef]:
#+begin_example
accuracy: 0.9083473154362416
                   pre       rec       spe        f1       geo       iba       sup

          0       0.99      0.88      0.94      0.93      0.91      0.82       149
          1       0.45      0.94      0.88      0.61      0.91      0.83        16

avg / total       0.94      0.88      0.93      0.90      0.91      0.82       165
#+end_example

The score skyrocketed for the majority class but it is still low for the minority class (0.45).

** Synthetic Minority Oversampling Technique
:PROPERTIES:
:CUSTOM_ID: synthetic-minority-oversampling-technique
:END:

SMOTE is another oversampling approach to deal with unbalanced datasets, here the size of the minority is also increased. In SMOTE, instead of selecting observations twice, new observations are created by interpolation.

#+begin_src python
from imblearn.over_sampling import SMOTE

X_resampled, y_resampled = SMOTE(
    random_state=1,
    sampling_strategy='auto'
).fit_resample(X_train, y_train)
print(X_resampled.shape, y_resampled.shape)
#+end_src

#+RESULTS[6f16c04c2d913872939572dddce5e9247680ad72]:
#+begin_example
(902, 2) (902,)
#+end_example

We will repeat the steps for making a prediction but with the new model.

#+begin_src python
model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)

y_pred = model.predict(X_test)
balanced_accuracy_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

print(classification_report_imbalanced(y_test, y_pred))
#+end_src

#+RESULTS[b8a02e9b9b64899022eadababe023ae05ad18bcb]:
#+begin_example
                   pre       rec       spe        f1       geo       iba       sup

          0       0.99      0.89      0.94      0.94      0.91      0.83       149
          1       0.48      0.94      0.89      0.64      0.91      0.84        16

avg / total       0.94      0.90      0.93      0.91      0.91      0.83       165
#+end_example

The results in this case are slighlty better but not by much. Oversampling techniques cannot overcome the deficiencies of the original dataset.

* Undersampling
:PROPERTIES:
:CUSTOM_ID: undersampling
:END:

Instead of increasing the number of the minority class, the size of the majority class is decreased.

Oversampling addresses class imbalance by duplicating or mimicking existing data, in contrast, undersampling only uses actual data. However, undersampling involves loss of data of the majority class so it is only practical with enough data in the dataset.

** Random Undersampling
:PROPERTIES:
:CUSTOM_ID: random-undersampling
:END:

#+begin_src python
import pandas as pd
from pathlib import Path
from collections import Counter

data = Path('../resources/cc_default.csv')
df = pd.read_csv(data)
print(df.head())
#+end_src

#+RESULTS[58fa5aa9a371bce2490a2bf7426d3fa0dc380d90]:
#+begin_example
   ID  ln_balance_limit  ...  age  default_next_month
0   1          9.903488  ...   24                   1
1   2         11.695247  ...   26                   1
2   3         11.407565  ...   34                   0
3   4         10.819778  ...   37                   0
4   5         10.819778  ...   57                   0

[5 rows x 7 columns]
#+end_example

Now we prepare the data.

#+begin_src python
from sklearn.model_selection import train_test_split

x_cols = [i for i in df.columns if i not in ('ID', 'default_next_month')]
X = df[x_cols]
y = df['default_next_month']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
print(X_train.shape, X_test.shape)
#+end_src

#+RESULTS[9d85b1ec0e1c8ce6b63f80c64379b9ffe1984c9f]:
#+begin_example
(22500, 5) (7500, 5)
#+end_example

We can use the =RandomUnderSampler= model.

#+begin_src python
from imblearn.under_sampling import RandomUnderSampler

ros = RandomUnderSampler(random_state=1)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)
print(Counter(y_resampled))
#+end_src

#+RESULTS[c328574ae46e0daae4bec1a91a39b8cb327c5b39]:
#+begin_example
Counter({0: 4968, 1: 4968})
#+end_example

Then we can make predictions.

#+begin_src python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
#+end_src

#+RESULTS[8516bf68ff372d76e873eb4c13542fde35ffe960]:
#+begin_example
[[3732 2100]
 [ 740  928]]
#+end_example

And finally we print the report.

#+begin_src python
from sklearn.metrics import balanced_accuracy_score
from imblearn.metrics import classification_report_imbalanced

print("accuracy:", balanced_accuracy_score(y_test, y_pred))
print(classification_report_imbalanced(y_test, y_pred))
#+end_src

#+RESULTS[ba8d625c33aba7ac4b5fb20a743dbb9b496e2eef]:
#+begin_example
accuracy: 0.5981363057701987
                   pre       rec       spe        f1       geo       iba       sup

          0       0.83      0.64      0.56      0.72      0.60      0.36      5832
          1       0.31      0.56      0.64      0.40      0.60      0.35      1668

avg / total       0.72      0.62      0.57      0.65      0.60      0.36      7500
#+end_example

The results are underwhelming.

** Cluster Centroid Undersampling
:PROPERTIES:
:CUSTOM_ID: cluster-centroid-undersampling
:END:

Similar to SMOTE, it generate synthetic data points, called centroids, that are representative of the clusters of other datapoints.

However, this algorithm is computationally intensive and may take a while to complete.

#+begin_src python
from imblearn.under_sampling import ClusterCentroids
from sklearn.linear_model import LogisticRegression

cc = ClusterCentroids(random_state=1)
X_resampled, y_resampled = cc.fit_resample(X_train, y_train)
model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)
print(model)
#+end_src

#+RESULTS[fbc3249fefafc829de0ced1521c498e9f7f7f614]:
#+begin_example
LogisticRegression(random_state=1)
#+end_example

Now we make a prediction.

#+begin_src python
y_pred = model.predict(X_test)
print(y_pred[:10])
#+end_src

#+RESULTS[a6bd88c24bc906665e333b9ec66c1660db1ae5c7]:
#+begin_example
[0 1 1 0 0 0 0 0 1 0]
#+end_example

And evaluate the model.

#+begin_src python
from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from imblearn.metrics import classification_report_imbalanced

cm = confusion_matrix(y_test, y_pred)
score = balanced_accuracy_score(y_test, y_pred)
print("Confusion Matrix:", cm)
print("accuracy:", score)
print(classification_report_imbalanced(y_test, y_pred))
#+end_src

#+RESULTS[81d37ca2e8d04a1d3a8dac90e98a1524e6107884]:
#+begin_example
Confusion Matrix: [[2840 2992]
 [ 612 1056]]
accuracy: 0.5600309875556345
                   pre       rec       spe        f1       geo       iba       sup

          0       0.82      0.49      0.63      0.61      0.56      0.30      5832
          1       0.26      0.63      0.49      0.37      0.56      0.31      1668

avg / total       0.70      0.52      0.60      0.56      0.56      0.31      7500
#+end_example

The results are even worse so we need to look for another option.

** SMOTEENN
:PROPERTIES:
:CUSTOM_ID: smoteenn
:END:

SMOTEENN combines both SMOTE and ENN algorithms:

1. Oversample th eminority class with SMOTE.
2. Clean the resulting data with undersampling. If the two nearest neighbors of a data point belong to two different classes, that data point is dropped.

So we are removing all synthetic datapoints that overlap between the two classes and are left with larger sample size than the original but not as much as the previous oversampling techniques. This gives us a more clean separation between classes.

#+begin_src python
import pandas as pd
from pathlib import Path
from collections import Counter

data = Path('../resources/cc_default.csv')
df = pd.read_csv(data)
print(df.head())
#+end_src

#+RESULTS[58fa5aa9a371bce2490a2bf7426d3fa0dc380d90]:
#+begin_example
   ID  ln_balance_limit  ...  age  default_next_month
0   1          9.903488  ...   24                   1
1   2         11.695247  ...   26                   1
2   3         11.407565  ...   34                   0
3   4         10.819778  ...   37                   0
4   5         10.819778  ...   57                   0

[5 rows x 7 columns]
#+end_example

We will prepare the data first.

#+begin_src python
from sklearn.model_selection import train_test_split

x_cols = [i for i in df.columns if i not in ('ID', 'default_next_month')]
X = df[x_cols]
y = df['default_next_month']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
print(X_train.shape, X_test.shape)
#+end_src

#+RESULTS[9d85b1ec0e1c8ce6b63f80c64379b9ffe1984c9f]:
#+begin_example
(22500, 5) (7500, 5)
#+end_example

Now we use SMOTEEN for resampling.

#+begin_src python
from imblearn.combine import SMOTEENN

smote_enn = SMOTEENN(random_state=0)
X_resampled, y_resampled = smote_enn.fit_resample(X, y)
print(X_resampled.shape, y_resampled.shape)
#+end_src

#+RESULTS[842b3f3445f484854ed7155830e7aec5e0666190]:
#+begin_example
(16441, 5) (16441,)
#+end_example

Then we make predictions.

#+begin_src python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='lbfgs', random_state=1)
model.fit(X_resampled, y_resampled)
y_pred = model.predict(X_test)
print(y_pred[:10])
#+end_src

#+RESULTS[c756bee923d770d7b0940edbb990bd6ee434087e]:
#+begin_example
[0 1 1 0 0 0 0 0 0 0]
#+end_example

And we evaluate the model.

#+begin_src python
from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from imblearn.metrics import classification_report_imbalanced

print("score:", balanced_accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report_imbalanced(y_test, y_pred))
#+end_src

#+RESULTS[27f016367aa10d010da78f47efc7b61bf96a2607]:
#+begin_example
score: 0.5671074251709743
[[4905  927]
 [1179  489]]
                   pre       rec       spe        f1       geo       iba       sup

          0       0.81      0.84      0.29      0.82      0.50      0.26      5832
          1       0.35      0.29      0.84      0.32      0.50      0.23      1668

avg / total       0.70      0.72      0.42      0.71      0.50      0.25      7500
#+end_example

The results are better than with undersampling although they are still not great.


* Footnotes
:PROPERTIES:
:CUSTOM_ID: footnotes
:END:

[fn:7]https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
[fn:6]https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html

[fn:5]https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
[fn:4]https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html

[fn:3]https://en.wikipedia.org/wiki/Logit
[fn:2]https://en.wikipedia.org/wiki/Sigmoid_function

[fn:1]https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
